# -*- coding: utf-8 -*-
"""4(1)_test_converting_a_checklist_that_has_a_table_with_many_test_results.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bO7JpO1y91MmJkStJIn7fOu8BD17-zdv

#MILESTONE 4:

Development: Automate cross-checking between table results and relevant checklist questions

Increase questions extraction from checklist tables by incorporating specific domain knowledge (Nuclear Density - Ground Compaction).
"""

!pip install easyocr pdf2image pytesseract opencv-python-headless Pillow

import cv2
import numpy as np
from pdf2image import convert_from_bytes
from PIL import Image
import pytesseract
import easyocr
import os
from google.colab import drive

drive.mount('/content/drive')

"""cloning the git repository to get the popplelr file to work with PDFs"""

!git clone https://anongit.freedesktop.org/git/poppler/poppler.git

!apt-get install poppler-utils
!sudo apt install tesseract-ocr
!sudo apt install libtesseract-dev
import os
os.environ['PATH'] += os.pathsep + os.path.expanduser("~/anongit/freedesktop.org/git/poppler/poppler/build/utils")



pdf_file_path_drive = '/content/drive/MyDrive/1-1_219P1020_CNPI25E_Cleanliness_and_Drying_Summary_28210.pdf'


with open(pdf_file_path_drive, "rb") as f:
    pdf_bytes = f.read()

pages = convert_from_bytes(pdf_bytes)
print(f"Converted {len(pages)} pages from the PDF to images.")

!sudo apt install tesseract-ocr
!sudo apt install libtesseract-dev


pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'



reader = easyocr.Reader(['en'], verbose=False)
extracted_questions = []


for i, page in enumerate(pages):
    print(f"\nProcessing Page {i + 1}...")

    img = np.array(page)
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    edges = cv2.Canny(gray, 50, 150)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
    dilated = cv2.dilate(edges, kernel, iterations=1)

    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)

        if w > 100 and h > 50:
            table_img = img[y:y + h, x:x + w]

            table_text = pytesseract.image_to_string(table_img)

            if "task" in table_text.lower():
                print("Detected Table:")
                display(Image.fromarray(cv2.cvtColor(table_img, cv2.COLOR_BGR2RGB)))
                print("Extracted Text:")
                print(table_text)

                lines = table_text.split("\n")
                questions = [line.strip() for line in lines if line.strip()]
                extracted_questions.extend(questions)

"""## Nuclear Density - procedure & activities"""

reader = easyocr.Reader(['en'])

def process_image_and_display_results(file):
    img = Image.open(file)
    img = np.array(img)
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 50, 150)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
    dilated = cv2.dilate(edges, kernel, iterations=1)

    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    st.subheader("Detection Results:")

    detected_tables = []
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        if w > 100 and h > 50:
            roi = img[y:y + h, x:x + w]
            table_text = reader.readtext(roi)

            if table_text:
                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)
                detected_text = " ".join([text[1] for text in table_text])
                detected_text = normalize_text(detected_text)
                detected_tables.append(detected_text)
                st.write(f"Detected Table: {detected_text}")

    st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), caption="Annotated Image with Detected Tables", use_container_width=True)

    if not detected_tables:
        st.warning("No tables detected.")

    return detected_tables

def normalize_text(text):
    text = text.replace("\n", " ")
    text = text.strip()
    text = text.lower()
    text = text.capitalize()
    return text

def generate_audio_from_text(table_texts, output_filename):
    combined_audio = AudioSegment.silent(duration=1000)

    for table_text in table_texts:
        rows = table_text.split(".")

        for row in rows:
            row = row.strip()
            if row:
                tts = gTTS(text=row, lang='en')
                temp_file = tempfile.NamedTemporaryFile(delete=True, suffix=".mp3")
                tts.save(temp_file.name)
                audio_segment = AudioSegment.from_mp3(temp_file.name)
                combined_audio += audio_segment
                combined_audio += AudioSegment.silent(duration=5000)

        tts_section = gTTS(text="SECTION IS COMPLETED", lang='en')
        temp_section_file = tempfile.NamedTemporaryFile(delete=True, suffix=".mp3")
        tts_section.save(temp_section_file.name)
        section_completed_audio = AudioSegment.from_mp3(temp_section_file.name)
        combined_audio += section_completed_audio
        combined_audio += AudioSegment.silent(duration=10000)

    combined_audio.export(output_filename, format="mp3")

st.title("Checklist - procedure activities")

uploaded_file = st.file_uploader("Upload an Image", type=["jpeg", "jpg", "png"])

if uploaded_file is not None:
    st.write("Processing your image...")
    extracted_tables = process_image_and_display_results(uploaded_file)

    if extracted_tables:
        st.write("Generating MP3 file with detected tables...")
        mp3_filename = "extracted_tables.mp3"
        generate_audio_from_text(extracted_tables, mp3_filename)
        st.success(f"MP3 file '{mp3_filename}' generated successfully!")
        st.audio(mp3_filename)
    else:
        st.warning("No tables were extracted to convert to audio.")

"""## Ground Compaction - levels & units"""

import easyocr
import cv2
import numpy as np
import streamlit as st
from gtts import gTTS
import time
import os
from pydub import AudioSegment

reader = easyocr.Reader(['en'])

def extract_text_from_image(image_path):
    image = cv2.imread(image_path)
    results = reader.readtext(image)

    extracted_text = [res[1] for res in results]
    return extracted_text

def process_text_to_sections(text_lines):
    sections = {}
    current_section = "GENERAL"
    sections[current_section] = []

    for line in text_lines:
        if line.isupper():
            current_section = line
            sections[current_section] = []
        else:
            sections[current_section].append(line)

    return sections

def generate_speech(sections):
    silence_5s = AudioSegment.silent(duration=5000)
    silence_10s = AudioSegment.silent(duration=10000)

    final_audio = AudioSegment.empty()

    for section, rows in sections.items():
        section_speech = gTTS(f"{section} section starts.", lang="en")
        section_file = "temp_section.mp3"
        section_speech.save(section_file)
        section_audio = AudioSegment.from_file(section_file)

        final_audio += section_audio

        if rows:
            for row in rows:
                row_speech = gTTS(row, lang="en")
                row_file = "temp_row.mp3"
                row_speech.save(row_file)
                row_audio = AudioSegment.from_file(row_file)
                final_audio += row_audio + silence_5s
                os.remove(row_file)

        section_complete = gTTS("Section complete.", lang="en")
        section_complete_file = "temp_complete.mp3"
        section_complete.save(section_complete_file)
        section_complete_audio = AudioSegment.from_file(section_complete_file)

        final_audio += section_complete_audio + silence_10s

        os.remove(section_file)
        os.remove(section_complete_file)

    output_file = "output.mp3"
    final_audio.export(output_file, format="mp3")
    return output_file

st.title("Checklist - Levels & Units")
uploaded_file = st.file_uploader("Upload an image", type=["png", "jpg", "jpeg"])

if uploaded_file is not None:
    image_path = "temp_image.png"
    with open(image_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    extracted_text = extract_text_from_image(image_path)
    st.text_area("Extracted Text:", "\n".join(extracted_text), height=300)

    sections = process_text_to_sections(extracted_text)

    mp3_file = generate_speech(sections)

    st.audio(mp3_file, format="audio/mp3")
    st.download_button("Download Speech", mp3_file)