# -*- coding: utf-8 -*-
"""2(8)_MP3_checklist_answers_Technician_Scenarios.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bs4hLDZ7ZnsZR0kRVXsOEpA29kABDJlD

# Milestone 2: AI PROJECT - Test MP3 checklist answers – Voice to MP3 creation from the field technician using field device (tablet or cell phone).

## Desired Outcome:
Should work in loud environments.
Test pausing a checklist. Field Techs should be able to stop and restart a checklist.
Test various audio devices.

– MILESTONE: Evaluate dataset quality and prepare processed data for topic modeling
– TASK: Ensure MP3 recordings maintain clarity in noisy conditions.
"""

!pip install noisereduce librosa scipy numpy

!pip install -q openai-whisper
!pip install -q torch torchaudio --index-url https://download.pytorch.org/whl/cu118
!apt-get install -qq ffmpeg

!pip install --upgrade numba

import noisereduce as nr
import librosa
import librosa.display
import numpy as np
import soundfile as sf


file_path = "/content/AUD_20250313_104848.m4a"
y, sr = librosa.load(file_path, sr=None)


noise_part = y[:sr]
reduced_audio = nr.reduce_noise(y=y, sr=sr, y_noise=noise_part)


sf.write("cleaned_audio.wav", reduced_audio, sr)
print("Noise reduction applied. Saved as 'cleaned_audio.wav'")

"""– MILESTONE: Perform cross-validation and compare different NLP models
– TASK: Compare multiple speech recognition models for accuracy.

"""

import whisper
model = whisper.load_model("small")
result = model.transcribe("cleaned_audio.wav")
print(f"Transcribed Text: {result['text']}")

!pip install transformers torchaudio

import torch
import torchaudio
from transformers import AutoProcessor, Wav2Vec2ForCTC


audio_path = "/content/cleaned_audio.wav"


device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "facebook/wav2vec2-large-960h"
processor = AutoProcessor.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name).to(device)

waveform, sample_rate = torchaudio.load(audio_path)
waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)
waveform = waveform.mean(dim=0, keepdim=True)


input_values = processor(waveform.squeeze(), return_tensors="pt", sampling_rate=16000).input_values.to(device)


with torch.no_grad():
    logits = model(input_values).logits


predicted_ids = torch.argmax(logits, dim=-1)
wav2vec_transcription = processor.batch_decode(predicted_ids)[0]


print("\n[WAV2VEC2 TRANSCRIPTION]:", wav2vec_transcription)

"""– MILESTONE: Validate sentiment and thematic analysis results against benchmark datasets
– TASK: Ensure MP3-generated responses match text answers.
"""

!pip install jiwer

import jiwer
from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

ground_truth = "What is the project? Alpha. What is the type of document? Bravo. What is the operation? Charly. What is the type of equipment? Delta. What is the label of equipment or tag of the process? Echo."
whisper_transcription = result['text']

def preprocess_transcription(transcription):
    return transcription.lower().replace("  ", " ").replace("\n", " ")

ground_truth = preprocess_transcription(ground_truth)
whisper_transcription = preprocess_transcription(whisper_transcription)
wav2vec2_transcription = preprocess_transcription(wav2vec_transcription)


wer = jiwer.wer(ground_truth, whisper_transcription)
print(f"Word Error Rate (WER) for Whisper: {wer}")

wer_wav2vec2 = jiwer.wer(ground_truth, wav2vec2_transcription)
print(f"Word Error Rate (WER) for Wav2Vec2: {wer_wav2vec2}")

cer = jiwer.cer(ground_truth, whisper_transcription)
print(f"Character Error Rate (CER) for Whisper: {cer}")

cer_wav2vec2 = jiwer.cer(ground_truth, wav2vec2_transcription)
print(f"Character Error Rate (CER) for Wav2Vec2: {cer_wav2vec2}")

def sentence_error_rate(ground_truth, transcription):
    gt_sentences = ground_truth.split(". ")
    trans_sentences = transcription.split(". ")

    errors = sum([gt != trans for gt, trans in zip(gt_sentences, trans_sentences)])
    return errors / len(gt_sentences)

ser_whisper = sentence_error_rate(ground_truth, whisper_transcription)
print(f"Sentence Error Rate (SER) for Whisper: {ser_whisper}")

ser_wav2vec2 = sentence_error_rate(ground_truth, wav2vec2_transcription)
print(f"Sentence Error Rate (SER) for Wav2Vec2: {ser_wav2vec2}")




ground_truth_words = ground_truth.split()
whisper_words = whisper_transcription.split()
wav2vec2_words = wav2vec2_transcription.split()

all_words = set(ground_truth_words + whisper_words + wav2vec2_words)

ground_truth_vector = [1 if word in ground_truth_words else 0 for word in all_words]
whisper_vector = [1 if word in whisper_words else 0 for word in all_words]
wav2vec2_vector = [1 if word in wav2vec2_words else 0 for word in all_words]

precision_whisper = precision_score(ground_truth_vector, whisper_vector, average='micro', zero_division=0)
recall_whisper = recall_score(ground_truth_vector, whisper_vector, average='micro', zero_division=0)
f1_whisper = f1_score(ground_truth_vector, whisper_vector, average='micro', zero_division=0)

print(f"Precision for Whisper: {precision_whisper}")
print(f"Recall for Whisper: {recall_whisper}")
print(f"F1 Score for Whisper: {f1_whisper}")



precision_wav2vec2 = precision_score(ground_truth_vector, wav2vec2_vector, average='micro', zero_division=0)
recall_wav2vec2 = recall_score(ground_truth_vector, wav2vec2_vector, average='micro', zero_division=0)
f1_wav2vec2 = f1_score(ground_truth_vector, wav2vec2_vector, average='micro', zero_division=0)

print(f"Precision for Wav2Vec2: {precision_wav2vec2}")
print(f"Recall for Wav2Vec2: {recall_wav2vec2}")
print(f"F1 Score for Wav2Vec2: {f1_wav2vec2}")